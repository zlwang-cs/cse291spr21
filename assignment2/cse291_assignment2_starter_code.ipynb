{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cse291_assignment2_starter_code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHnbGR5wpCL8"
      },
      "source": [
        "# CSE 291 Assignment 2 BiLSTM CRF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs2O4920pCob"
      },
      "source": [
        "## Download Data/Eval Script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmfarI0hpHj6"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\n",
        "!wget https://raw.githubusercontent.com/tberg12/cse291spr21/main/assignment2/train.data.quad\n",
        "!wget https://raw.githubusercontent.com/tberg12/cse291spr21/main/assignment2/dev.data.quad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CMvXrmwpNCM"
      },
      "source": [
        "import conlleval\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.vocab import Vocab\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "torch.manual_seed(291)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOBmqHytpTGs"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKfmSZs8pPBV"
      },
      "source": [
        "TRAIN_DATA = 'train.data.quad'\n",
        "VALID_DATA = 'dev.data.quad'\n",
        "UNK = '<unk>'\n",
        "PAD = '<pad>'\n",
        "START_TAG = \"<start>\"  # you can add this explicitly or use it implicitly in your CRF layer\n",
        "STOP_TAG = \"<stop>\"    # you can add this explicitly or use it implicitly in your CRF layer\n",
        "\n",
        "\n",
        "def read_conll_sentence(path):\n",
        "    \"\"\" Read a CONLL-format sentence into vocab objects\n",
        "    Args:\n",
        "        :param path: path to CONLL-format data file\n",
        "        :param word_vocab: Vocabulary object for source\n",
        "        :param label_vocab: Vocabulary object for target\n",
        "    \"\"\"\n",
        "    sent = [[], []]\n",
        "    with open(path) as f:\n",
        "        for line in f:\n",
        "            line = line.strip().split()\n",
        "            if line:\n",
        "                # replace numbers with 0000\n",
        "                word = line[0]\n",
        "                word = '0000' if word.isnumeric() else word\n",
        "                sent[0].append(word)\n",
        "                sent[1].append(line[3])\n",
        "            else:\n",
        "                yield sent[0], sent[1]\n",
        "                sent = [[], []]\n",
        "\n",
        "\n",
        "def prepare_dataset(dataset, word_vocab, label_vocab):\n",
        "    dataset = [\n",
        "      [\n",
        "        torch.tensor([word_vocab.stoi[word] for word in sent[0]], dtype=torch.long),\n",
        "        torch.tensor([label_vocab.stoi[label] for label in sent[1]], dtype=torch.long),\n",
        "      ]\n",
        "      for sent in dataset\n",
        "    ]\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# load a list of sentences, where each word in the list is a tuple containing the word and the label\n",
        "train_data = list(read_conll_sentence(TRAIN_DATA))\n",
        "train_word_counter = Counter([word for sent in train_data for word in sent[0]])\n",
        "train_label_counter = Counter([label for sent in train_data for label in sent[1]])\n",
        "word_vocab = Vocab(train_word_counter, specials=(UNK, PAD), min_freq=2)\n",
        "label_vocab = Vocab(train_label_counter, specials=(), min_freq=1)\n",
        "train_data = prepare_dataset(train_data, word_vocab, label_vocab)\n",
        "print('Train word vocab:', len(word_vocab), 'symbols.')\n",
        "print('Train label vocab:', len(label_vocab), f'symbols: {list(label_vocab.stoi.keys())}')\n",
        "valid_data = list(read_conll_sentence(VALID_DATA))\n",
        "valid_data = prepare_dataset(valid_data, word_vocab, label_vocab)\n",
        "print('Train data:', len(train_data), 'sentences.')\n",
        "print('Valid data:', len(valid_data))\n",
        "\n",
        "print(' '.join([word_vocab.itos[i.item()] for i in train_data[0][0]]))\n",
        "print(' '.join([label_vocab.itos[i.item()] for i in train_data[0][1]]))\n",
        "\n",
        "print(' '.join([word_vocab.itos[i.item()] for i in valid_data[1][0]]))\n",
        "print(' '.join([label_vocab.itos[i.item()] for i in valid_data[1][1]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNNmZx_Uqy7q"
      },
      "source": [
        "## BiLSTMTagger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5nVIM_Eq1ZU"
      },
      "source": [
        "# Starter code implementing a BiLSTM Tagger\n",
        "# which makes locally normalized, independent\n",
        "# tag classifications at each time step\n",
        "\n",
        "class BiLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tag_vocab_size, embedding_dim, hidden_dim, dropout=0.3):\n",
        "        super(BiLSTMTagger, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tagset_size = tag_vocab_size\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim).to(device)\n",
        "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True, batch_first=True).to(device)\n",
        "        self.tag_projection_layer = nn.Linear(hidden_dim, self.tagset_size).to(device)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2).to(device),\n",
        "                torch.randn(2, 1, self.hidden_dim // 2).to(device))\n",
        "\n",
        "    def compute_lstm_emission_features(self, sentence):\n",
        "        hidden = self.init_hidden()\n",
        "        embeds = self.dropout(self.word_embeds(sentence))\n",
        "        bilstm_out, hidden = self.bilstm(embeds, hidden)\n",
        "        bilstm_out = self.dropout(bilstm_out)\n",
        "        bilstm_out = bilstm_out\n",
        "        bilstm_feats = self.tag_projection_layer(bilstm_out)\n",
        "        return bilstm_feats\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        bilstm_feats = self.compute_lstm_emission_features(sentence)\n",
        "        return bilstm_feats.max(-1)[0].sum(), bilstm_feats.argmax(-1)\n",
        "\n",
        "    def loss(self, sentence, tags):\n",
        "        bilstm_feats = self.compute_lstm_emission_features(sentence)\n",
        "        # transform predictions to (n_examples, n_classes) and ground truth to (n_examples)\n",
        "        return torch.nn.functional.cross_entropy(\n",
        "              bilstm_feats.view(-1, self.tagset_size), \n",
        "              tags.view(-1), \n",
        "              reduction='sum'\n",
        "            )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH7JGSDAruUg"
      },
      "source": [
        "## Train / Eval loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw2He2cgrrF1"
      },
      "source": [
        "def train(model, train_data, valid_data, word_vocab, label_vocab, epochs, log_interval=25):\n",
        "    losses_per_epoch = []\n",
        "    for epoch in range(epochs):\n",
        "        print(f'--- EPOCH {epoch} ---')\n",
        "        model.train()\n",
        "        losses_per_epoch.append([])\n",
        "        for i, (sent, tags) in enumerate(train_data):\n",
        "            model.zero_grad()\n",
        "            sent, tags = sent.to(device), tags.to(device)\n",
        "            sent = sent.unsqueeze(0)\n",
        "            tags = tags.unsqueeze(0)\n",
        "            loss = model.loss(sent, tags)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            losses_per_epoch[-1].append(loss.detach().cpu().item())\n",
        "            if i > 0 and i % log_interval == 0:\n",
        "                print(f'Avg loss over last {log_interval} updates: {np.mean(losses_per_epoch[-1][-log_interval:])}')\n",
        "\n",
        "        evaluate(model, valid_data, word_vocab, label_vocab)\n",
        "\n",
        "\n",
        "def evaluate(model, dataset, word_vocab, label_vocab):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    scores = []\n",
        "    true_tags = []\n",
        "    pred_tags = []\n",
        "    sents = []\n",
        "    for i, (sent, tags) in enumerate(dataset):\n",
        "        with torch.no_grad():\n",
        "            sent, tags = sent.to(device), tags.to(device)\n",
        "            sent = sent.unsqueeze(0)\n",
        "            tags = tags.unsqueeze(0)\n",
        "            losses.append(model.loss(sent, tags).cpu().detach().item())\n",
        "            score, pred_tag_seq = model(sent)\n",
        "            scores.append(score.cpu().detach().numpy())\n",
        "            true_tags.append([label_vocab.itos[i] for i in tags.tolist()[0]])\n",
        "            pred_tags.append([label_vocab.itos[i] for i in pred_tag_seq[0]])\n",
        "            sents.append([word_vocab.itos[i] for i in sent[0]])\n",
        "\n",
        "    print('Avg evaluation loss:', np.mean(losses))\n",
        "    print(conlleval.evaluate([tag for tags in true_tags for tag in tags], [tag for tags in pred_tags for tag in tags], verbose=True))\n",
        "    print('\\n5 random evaluation samples:')\n",
        "    for i in np.random.randint(0, len(sents), size=5):\n",
        "        print('SENT:', ' '.join(sents[i]))\n",
        "        print('TRUE:', ' '.join(true_tags[i]))\n",
        "        print('PRED:', ' '.join(pred_tags[i]))\n",
        "    return sents, true_tags, pred_tags\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdJsc_y6rxdC"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVyfoJfZry4-"
      },
      "source": [
        "# Train BiLSTM Tagger Baseline\n",
        "model = BiLSTMTagger(len(word_vocab), len(label_vocab), 128, 256).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "train(model, train_data, valid_data, word_vocab, label_vocab, epochs=30, log_interval=500)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
